{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import get_random_walk_env, get_four_rooms_env\n",
    "from policy import equiprobable_policy, greedy_epsilon_policy\n",
    "from algo import td0, mc_prediction, generate_episode, on_policy_mc_control_fv, off_policy_mc_prediction, on_policy_mc_Q_prediction\n",
    "from utils import load_or_generate_data\n",
    "from visualize import plot_policy_on_grid_world, plot_grid_world_value_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-bright')\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, threshold=1000, linewidth=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_BASE_DIR = 'figures/'\n",
    "CACHE_BASE_DIR = 'cache/'\n",
    "\n",
    "if not os.path.exists(FIG_BASE_DIR):\n",
    "    os.makedirs(FIG_BASE_DIR)\n",
    "\n",
    "if not os.path.exists(CACHE_BASE_DIR):\n",
    "    os.makedirs(CACHE_BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment + initial value function\n",
    "env = get_random_walk_env()\n",
    "initial_V = np.zeros(env.observation_space.n)\n",
    "initial_V[1:-1] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_T0_ESTIMATES_FIG = os.path.join(FIG_BASE_DIR, 'V_T0_estimates.png')\n",
    "true_V = np.arange(1, 6) / 6\n",
    "\n",
    "if os.path.exists(V_T0_ESTIMATES_FIG):\n",
    "    # load and plot png file\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(plt.imread(V_T0_ESTIMATES_FIG))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    V_T0, estimates_over_time_T0 = td0(env, equiprobable_policy(env), alpha=0.1, gamma=1, n_episodes=101, initial_V=initial_V, verbose=True)\n",
    "\n",
    "    episodes_of_interest = [0, 1, 10, 100]\n",
    "    x = np.arange(1, 6)\n",
    "    line_colors = ['black', 'red', 'green', 'blue']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, ep_idx in enumerate(episodes_of_interest):\n",
    "        plt.plot(x, estimates_over_time_T0[ep_idx][1:6], label=f'Episode {ep_idx}', color=line_colors[i], marker='o', linewidth=1)\n",
    "    plt.plot(x,true_V, label='True V', color='black', linestyle='--', marker='o', linewidth=1)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(np.arange(1, 6), ['A', 'B', 'C', 'D', 'E'])\n",
    "    plt.legend()\n",
    "    plt.title('TD(0) prediction')\n",
    "    plt.savefig(V_T0_ESTIMATES_FIG)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(V1, V2):\n",
    "    \"\"\"\n",
    "    Simple function to calculate the root mean squared error between two value functions\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((V1 - V2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_VS_TD0_RMSE_FIG = os.path.join(FIG_BASE_DIR, 'MC_vs_TD0_RMSE.png')\n",
    "\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "alphas_MC = [0.04, 0.03, 0.02, 0.01]\n",
    "alphas_T0 = [0.15, 0.1, 0.05]\n",
    "NUM_EPISODES = 101\n",
    "NUM_RUNS = 100\n",
    "\n",
    "if os.path.exists(MC_VS_TD0_RMSE_FIG):\n",
    "    # load and plot png file\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(plt.imread(MC_VS_TD0_RMSE_FIG))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, alpha in enumerate(alphas_MC):\n",
    "        V_over_time = np.zeros((NUM_RUNS, NUM_EPISODES, env.observation_space.n))\n",
    "        for r in trange(NUM_RUNS):\n",
    "            V, V_over_time[r] = mc_prediction(env, equiprobable_policy(env), alpha=alpha, gamma=1, n_episodes=NUM_EPISODES, initial_V=initial_V)\n",
    "        rmse_over_time = np.apply_along_axis(lambda x: rmse(x[1:6], true_V), 2, V_over_time)\n",
    "        rmse_over_time = np.mean(rmse_over_time, axis=0)\n",
    "        plt.plot(rmse_over_time, label=f'MC $\\\\alpha={alpha}$', linewidth=1, color='red', linestyle=linestyles[i])\n",
    "\n",
    "    for i, alpha in enumerate(alphas_T0):\n",
    "        V_over_time = np.zeros((NUM_RUNS, NUM_EPISODES, env.observation_space.n))\n",
    "        for r in trange(NUM_RUNS):\n",
    "            V, V_over_time[r] = td0(env, equiprobable_policy(env), alpha=alpha, gamma=1, n_episodes=NUM_EPISODES, initial_V=initial_V)\n",
    "        rmse_over_time = np.apply_along_axis(lambda x: rmse(x[1:6], true_V), 2, V_over_time)\n",
    "        rmse_over_time = np.mean(rmse_over_time, axis=0)\n",
    "        plt.plot(rmse_over_time, label=f'TD(0) $\\\\alpha={alpha}$', linewidth=1, color='blue', linestyle=linestyles[i])\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.title('Empirical RMSE averaged over states for TD(0) and MC')\n",
    "    plt.savefig(MC_VS_TD0_RMSE_FIG)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Off-policy vs On-policy Comparison\n",
    "\n",
    "Some setup for future experiments.\n",
    "\n",
    "1. `random_policy_10k_episodes` - a list of 10k episodes generated by a random equiprobable policy.\n",
    "2. `mc_fv_Q` - Q state-action value function generated by MC first-visit on-policy control. Policy here is an epsilon-greedy policy with epsilon=0.1.\n",
    "3. `mc_fv_episodes` - the episodes generated while training `mc_fv_Q`.\n",
    "\n",
    "**Important Note**: Our discount rate is `0.99`, and our epsilon for the epsilon-greedy policy is `0.1`. With this combo, we may expect that our epsilon-greedy policy will be greedy most of the time. Since, discount is so high and environment so small, we may get stuck in a suboptimal path (as long as it ends in a reward).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_four_rooms_env()\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_POLICY_10k_EPISODES_PATH = os.path.join(CACHE_BASE_DIR, 'random_policy_10k_episodes.pkl')\n",
    "ON_POLICY_MC_Q_AND_EPISODES_PATH = os.path.join(CACHE_BASE_DIR, 'on_policy_mc_q_and_episodes.pkl')\n",
    "\n",
    "\n",
    "def run_10k_episodes_rand_policy(env):\n",
    "    random_policy_10k_episodes = []\n",
    "    random_policy = equiprobable_policy(env)\n",
    "    for i in range(10_000):\n",
    "        random_policy_10k_episodes.append(generate_episode(env, random_policy))\n",
    "    \n",
    "    return random_policy_10k_episodes\n",
    "\n",
    "\n",
    "random_policy_10k_episodes = load_or_generate_data(RANDOM_POLICY_10k_EPISODES_PATH, run_10k_episodes_rand_policy, env)\n",
    "mc_fv_Q, mc_fv_episodes = load_or_generate_data(ON_POLICY_MC_Q_AND_EPISODES_PATH, on_policy_mc_control_fv, env, lambda Q: greedy_epsilon_policy(env, Q, EPSILON), gamma=GAMMA, n_episodes=10_000, verbose=True, extractor=lambda x: (dict(x[2]), x[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pi_greedy` - is what I will call the greedy policy derived from `mc_fv_Q`. For state that were not visited, we will assume that the policy is equiprobable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_greedy = greedy_epsilon_policy(env, mc_fv_Q, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the greedy policy derived from MC first-visit on-policy control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_ON_POLICY_MC_CONTROL_FIG = os.path.join(FIG_BASE_DIR, 'on_policy_mc_control.png')\n",
    "plot_policy_on_grid_world(env, mc_fv_Q, title='$\\\\pi_{Greedy}$ from on-policy MC control (10k episodes)', save_path=MC_ON_POLICY_MC_CONTROL_FIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off Policy MC Prediction\n",
    "\n",
    "Using `mc_fv_episodes` and `random_policy_10k_episodes`, we will compute two new $Q_\\pi$ estimates. One with $\\pi_{greedy}$ and one with $\\pi_{random}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature -> off_policy_mc_prediction(env: Env, gamma: float, n_episodes: int, pre_generated_eps: List, verbose: bool = False):\n",
    "RANDOM_OFF_POLICY_ESTIMATES_PATH = os.path.join(CACHE_BASE_DIR, 'random_off_policy_mc_estimates.pkl')\n",
    "SOFT_EPSILON_GREEDY_OFF_POLICY_ESTIMATES_PATH = os.path.join(CACHE_BASE_DIR, 'soft_epsilon_greedy_off_policy_mc_estimates.pkl')\n",
    "\n",
    "# rand_off_policy_mc_Q, rand_off_policy_mc_V = load_or_generate_data(RANDOM_OFF_POLICY_ESTIMATES_PATH, off_policy_mc_prediction, env, GAMMA, random_policy_10k_episodes, verbose=True)\n",
    "# greedy_off_policy_mc_Q, greedy_off_policy_mc_V = load_or_generate_data(SOFT_EPSILON_GREEDY_OFF_POLICY_ESTIMATES_PATH, off_policy_mc_prediction, env, GAMMA,  mc_fv_episodes, verbose=True)\n",
    "rand_off_policy_mc_Q, rand_off_policy_mc_V = off_policy_mc_prediction(env, GAMMA, random_policy_10k_episodes, verbose=True)\n",
    "greedy_off_policy_mc_Q, greedy_off_policy_mc_V = off_policy_mc_prediction(env, GAMMA, mc_fv_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_V_dict_to_array_grid_world(env, V_dict):\n",
    "    V_map = np.zeros((env.unwrapped.cols, env.unwrapped.rows))\n",
    "    for x in range(env.unwrapped.cols):\n",
    "        for y in range(env.unwrapped.rows):\n",
    "            V_map[y, x] = np.max(V_dict.get((x, y), 0))\n",
    "    flip_V_map = np.flipud(V_map) # flip y-axis to match grid world\n",
    "    return flip_V_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_OFF_POLICY_Q_FIG = os.path.join(FIG_BASE_DIR, 'random_off_policy_mc_V.png')\n",
    "plot_grid_world_value_function(env, rand_off_policy_mc_V, title='Off-Policy $V_\\\\pi$ (Random Behavior Policy)', save_path=RANDOM_OFF_POLICY_Q_FIG)\n",
    "convert_V_dict_to_array_grid_world(env, rand_off_policy_mc_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFT_EPSILON_GREEDY_OFF_POLICY_Q_FIG = os.path.join(FIG_BASE_DIR, 'soft_epsilon_greedy_off_policy_mc_V.png')\n",
    "plot_grid_world_value_function(env, greedy_off_policy_mc_V, title='Off-Policy $V_\\\\pi$ (Epsiodes to Train $\\\\pi_{greedy}$ Policy)', save_path=SOFT_EPSILON_GREEDY_OFF_POLICY_Q_FIG)\n",
    "convert_V_dict_to_array_grid_world(env, greedy_off_policy_mc_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Policy MC Prediction (using $\\pi_{greedy}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_policy_Q = on_policy_mc_Q_prediction(env, pi_greedy, gamma=GAMMA, n_episodes=10_000, verbose=True)\n",
    "on_policy_V = {state: np.max(action_values) for state, action_values in on_policy_Q.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_POLICY_GREEDY_PI_ESTIMATES_FIG = os.path.join(FIG_BASE_DIR, 'on_policy_greedy_V_pi_estimates.png')\n",
    "plot_grid_world_value_function(env, on_policy_V, title='On-Policy $V_\\\\pi$ ($\\\\pi_{greedy}$ Policy)', save_path=SOFT_EPSILON_GREEDY_OFF_POLICY_Q_FIG)\n",
    "convert_V_dict_to_array_grid_world(env, on_policy_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
